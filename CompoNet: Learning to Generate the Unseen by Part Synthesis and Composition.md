# CompoNet: Learning to Generate the Unseen by Part Synthesis and Composition

## Summary
The proposed CompoNet learns to synthesize shapes that can be represented as a composition of distinct parts.
## Research Objective
- How to enable a generative model to go beyond the observed observed samples and learn to generate "unseen".
## Background and Problems
- Existing generative models are always trapped or bounded by the empirical distribution of the observed data.
## Methods
- The proposed CompoNet is applied both on 2D and 3D shape. At training time, every shape is pre-segmented to its semantic parts.
- Part synthesis unit.
	- Training an individual VAE for each semantic part.
	- The encoders are fixed during the training of part composition unit.
- Parts composition unit.
	- Codes generated by encoders are fed into a composition network which learns to produce transformation parameters per part.
	- For 2D shape, the network takes in (10x4+8)-dim vector, and outputs a 16-dim vector, four values per part.
	- For 3D shape(point cloud), the network takes in (64x4+16)-dim vector, and outputs a 24-dim vector, six values per part. 
## Evaluation
- 2D. Projected COSEG
- 3D. ShapeNet
## Conclusion
- Novel shapes are generated via inference over random samples taken from the latent spaces of shape parts and part compositions.
## Notes
- It does not allow changes to part structures or feature transfers between different part classes.
- This model uses a fixed number of semantic parts. For a missing part, they use null vector.
- Similar to PAGENet.
- [CompoNet](https://github.com/nschor/CompoNet)
## References


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTU3MzY5Mjc3Myw4NjE2OTIwNzAsLTEwMj
A4MTA2MzAsLTUxMDYxNTA1Niw3MzA5OTgxMTZdfQ==
-->